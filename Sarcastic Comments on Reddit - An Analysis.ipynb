{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nour Mansour and Juan Estrella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Data Collection\n",
    "\n",
    "Step 1: Get the tsv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0      0                                         NC and NH.\n",
       "1      0  You do know west teams play against west teams...\n",
       "2      0  They were underdogs earlier today, but since G...\n",
       "3      0  This meme isn't funny none of the \"new york ni...\n",
       "4      0                    I could use one of those tools."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data is saved in the same folder as the project. Then read data from tsv file\n",
    "data = pd.read_csv(\"train-balanced-sarcasm.csv\")\n",
    "data.dropna(inplace=True)\n",
    "data.drop(['author', 'score', 'ups', 'downs', 'date', 'created_utc', 'subreddit', 'parent_comment'], axis = 1, inplace = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Data Processing\n",
    "\n",
    "Step 1: Columns required: Label, Comments, subreddit, parent comment\n",
    "\n",
    "Step 2: Create a Dataframe containing an even amount of sarcastic and non sarcastic \n",
    "comments, amount of data is 505413 each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1010768</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm sure that Iran and N. Korea have the techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010769</th>\n",
       "      <td>1</td>\n",
       "      <td>whatever you do, don't vote green!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010770</th>\n",
       "      <td>1</td>\n",
       "      <td>Perhaps this is an atheist conspiracy to make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010771</th>\n",
       "      <td>1</td>\n",
       "      <td>The Slavs got their own country - it is called...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010772</th>\n",
       "      <td>1</td>\n",
       "      <td>values, as in capitalism .. there is good mone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                            comment\n",
       "1010768      1  I'm sure that Iran and N. Korea have the techn...\n",
       "1010769      1                 whatever you do, don't vote green!\n",
       "1010770      1  Perhaps this is an atheist conspiracy to make ...\n",
       "1010771      1  The Slavs got their own country - it is called...\n",
       "1010772      1  values, as in capitalism .. there is good mone..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Copy the only the data we need\n",
    "#required_data = data[['label','comment','subreddit','parent_comment']]\n",
    "#required_data.head()\n",
    "# Reset the indices after rows with NA values are dropped\n",
    "data.reset_index(inplace = True)\n",
    "data.drop(['index'], axis = 1, inplace = True)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>106915</th>\n",
       "      <td>0</td>\n",
       "      <td>Animal rights breh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376203</th>\n",
       "      <td>0</td>\n",
       "      <td>Baja la applicasion de Xfinity Tv to go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915045</th>\n",
       "      <td>0</td>\n",
       "      <td>Safety turd, in the hot tub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437900</th>\n",
       "      <td>0</td>\n",
       "      <td>Infinite hiatus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403426</th>\n",
       "      <td>0</td>\n",
       "      <td>Those institutions still have no lasting power...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                            comment\n",
       "0 106915      0                                 Animal rights breh\n",
       "  376203      0            Baja la applicasion de Xfinity Tv to go\n",
       "  915045      0                        Safety turd, in the hot tub\n",
       "  437900      0                                    Infinite hiatus\n",
       "  403426      0  Those institutions still have no lasting power..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rand_ind = np.random.choice(data.index, int(len(data)*10/100))\n",
    "#len(rand_ind) # 101077 for 10%\n",
    "\n",
    "#chosen_data = data.sample(frac=0.5, random_state=1)\n",
    "#chosen_data.tail()\n",
    "\n",
    "\n",
    "\n",
    "size = int(len(data)*5/100)# sample size\n",
    "replace = True  # with replacement\n",
    "fn = lambda obj: obj.loc[np.random.choice(obj.index, size, replace),:]\n",
    "chosen_data = data.groupby('label', as_index=False).apply(fn)\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Animal rights breh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Baja la applicasion de Xfinity Tv to go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Safety turd, in the hot tub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Infinite hiatus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Those institutions still have no lasting power...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0      0                                 Animal rights breh\n",
       "1      0            Baja la applicasion de Xfinity Tv to go\n",
       "2      0                        Safety turd, in the hot tub\n",
       "3      0                                    Infinite hiatus\n",
       "4      0  Those institutions still have no lasting power..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_data.reset_index(inplace = True)\n",
    "chosen_data.drop(['level_0', 'level_1'], inplace = True, axis = 1)\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101071</th>\n",
       "      <td>1</td>\n",
       "      <td>Because if she isn't the one doing the exposin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101072</th>\n",
       "      <td>1</td>\n",
       "      <td>if you dont count the mass genocide and corrup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101073</th>\n",
       "      <td>1</td>\n",
       "      <td>Screaming \"you faggot\" was a great touch.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101074</th>\n",
       "      <td>1</td>\n",
       "      <td>We all know that all PC gamers and WOW players...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101075</th>\n",
       "      <td>1</td>\n",
       "      <td>You commie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                            comment\n",
       "101071      1  Because if she isn't the one doing the exposin...\n",
       "101072      1  if you dont count the mass genocide and corrup...\n",
       "101073      1          Screaming \"you faggot\" was a great touch.\n",
       "101074      1  We all know that all PC gamers and WOW players...\n",
       "101075      1                                         You commie"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Exploratory Analysis & Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore distribution of the data by label (0 -> non-sarcastic, 1 -> sarcastic)\n",
    "plt.hist(chosen_data.label)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Comments by Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">comment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50538</td>\n",
       "      <td>46968</td>\n",
       "      <td>Thanks!</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50538</td>\n",
       "      <td>47271</td>\n",
       "      <td>You forgot the</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment                            \n",
       "        count unique             top freq\n",
       "label                                    \n",
       "0       50538  46968         Thanks!   35\n",
       "1       50538  47271  You forgot the  131"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore distribution of comments by label \n",
    "chosen_data.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/16/4d247e27c55a7b6412e7c4c86f2500ae61afcbf5932b9e3491f8462f8d9e/nltk-3.4.4.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 5.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/41/c8/31/48ace4468e236e0e8435f30d33e43df48594e4d53e367cf061\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.4\n"
     ]
    }
   ],
   "source": [
    "#Import NLTK library\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package rte to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | Downloading package rslp to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #Use this to get rid off meaningless words like \"the, and, a\"\n",
    "from nltk.tokenize import word_tokenize #Split by word\n",
    "from nltk.tokenize import sent_tokenize #Split by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure all the comment column is str data type\n",
    "chosen_data['comment'] = chosen_data['comment'].astype(str)\n",
    "chosen_data['comment'] = chosen_data['comment'].astype(str)\n",
    "\n",
    "#non_sarcastic = required_data.loc[required_data['label'] == 0]\n",
    "#sarcastic = required_data.loc[required_data['label'] == 1]\n",
    "\n",
    "#drop rows with na values on the comment column\n",
    "#non_sarcastic['comment'].dropna(inplace=True)\n",
    "#sarcastic['comment'].dropna(inplace=True)\n",
    "\n",
    "#Make sure all the comment column is str data type\n",
    "#non_sarcastic['comment'] = non_sarcastic['comment'].astype(str)\n",
    "#sarcastic['comment'] = sarcastic['comment'].astype(str)\n",
    "#print(len(non_sarcastic), len(sarcastic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>POS_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Animal, rights, breh]</td>\n",
       "      <td>[(Animal, NNP), (rights, NNS), (breh, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[Baja, la, applicasion, de, Xfinity, Tv, to, go]</td>\n",
       "      <td>[(Baja, NNP), (la, CC), (applicasion, NN), (de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[Safety, turd, ,, in, the, hot, tub]</td>\n",
       "      <td>[(Safety, NNP), (turd, NN), (,, ,), (in, IN), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[Infinite, hiatus]</td>\n",
       "      <td>[(Infinite, NNP), (hiatus, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[Those, institutions, still, have, no, lasting...</td>\n",
       "      <td>[(Those, DT), (institutions, NNS), (still, RB)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0                             [Animal, rights, breh]   \n",
       "1      0   [Baja, la, applicasion, de, Xfinity, Tv, to, go]   \n",
       "2      0               [Safety, turd, ,, in, the, hot, tub]   \n",
       "3      0                                 [Infinite, hiatus]   \n",
       "4      0  [Those, institutions, still, have, no, lasting...   \n",
       "\n",
       "                                             POS_tag  \n",
       "0         [(Animal, NNP), (rights, NNS), (breh, NN)]  \n",
       "1  [(Baja, NNP), (la, CC), (applicasion, NN), (de...  \n",
       "2  [(Safety, NNP), (turd, NN), (,, ,), (in, IN), ...  \n",
       "3                    [(Infinite, NNP), (hiatus, NN)]  \n",
       "4  [(Those, DT), (institutions, NNS), (still, RB)...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_data['comment'] = chosen_data['comment'].apply(word_tokenize)\n",
    "chosen_data['POS_tag'] = chosen_data['comment'].apply(nltk.pos_tag)\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Animal, rights, breh]</td>\n",
       "      <td>[(Animal, NNP), (rights, NNS), (breh, NN)]</td>\n",
       "      <td>[anim, right, breh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[Baja, la, applicasion, de, Xfinity, Tv, to, go]</td>\n",
       "      <td>[(Baja, NNP), (la, CC), (applicasion, NN), (de...</td>\n",
       "      <td>[baja, la, applicas, de, xfiniti, tv, to, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[Safety, turd, ,, in, the, hot, tub]</td>\n",
       "      <td>[(Safety, NNP), (turd, NN), (,, ,), (in, IN), ...</td>\n",
       "      <td>[safeti, turd, ,, in, the, hot, tub]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[Infinite, hiatus]</td>\n",
       "      <td>[(Infinite, NNP), (hiatus, NN)]</td>\n",
       "      <td>[infinit, hiatus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[Those, institutions, still, have, no, lasting...</td>\n",
       "      <td>[(Those, DT), (institutions, NNS), (still, RB)...</td>\n",
       "      <td>[those, institut, still, have, no, last, power...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0                             [Animal, rights, breh]   \n",
       "1      0   [Baja, la, applicasion, de, Xfinity, Tv, to, go]   \n",
       "2      0               [Safety, turd, ,, in, the, hot, tub]   \n",
       "3      0                                 [Infinite, hiatus]   \n",
       "4      0  [Those, institutions, still, have, no, lasting...   \n",
       "\n",
       "                                             POS_tag  \\\n",
       "0         [(Animal, NNP), (rights, NNS), (breh, NN)]   \n",
       "1  [(Baja, NNP), (la, CC), (applicasion, NN), (de...   \n",
       "2  [(Safety, NNP), (turd, NN), (,, ,), (in, IN), ...   \n",
       "3                    [(Infinite, NNP), (hiatus, NN)]   \n",
       "4  [(Those, DT), (institutions, NNS), (still, RB)...   \n",
       "\n",
       "                                                stem  \n",
       "0                                [anim, right, breh]  \n",
       "1      [baja, la, applicas, de, xfiniti, tv, to, go]  \n",
       "2               [safeti, turd, ,, in, the, hot, tub]  \n",
       "3                                  [infinit, hiatus]  \n",
       "4  [those, institut, still, have, no, last, power...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "chosen_data['stem'] = chosen_data['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_len = len(data)\n",
    "#data1 = data.iloc[0:(int(data_len/4))].copy()\n",
    "#data1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data2 = data.iloc[(int(data_len/4)):(int(data_len/2))].copy()\n",
    "#data2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data3 = data.iloc[(int(data_len/2)):(int(3*data_len/4))].copy()\n",
    "#data3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data4 = data.iloc[(int(3*data_len/4)):].copy()\n",
    "#data4.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data4['comment'] = data4['comment'].apply(word_tokenize)\n",
    "# data4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data4['POS_tag'] = data4['comment'].apply(nltk.pos_tag)\n",
    "# data4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data4['stem'] = data4['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# data4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data3['comment'] = data3['comment'].apply(word_tokenize)\n",
    "# data3['POS_tag'] = data3['comment'].apply(nltk.pos_tag)\n",
    "# data3['stem'] = data3['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2['comment'] = data2['comment'].apply(word_tokenize)\n",
    "# data2['POS_tag'] = data2['comment'].apply(nltk.pos_tag)\n",
    "# data2['stem'] = data2['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1['comment'] = data1['comment'].apply(word_tokenize)\n",
    "# data1['POS_tag'] = data1['comment'].apply(nltk.pos_tag)\n",
    "# data1['stem'] = data1['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the required data set in half to ease later comment transformations\n",
    "# df1 = data.iloc[0:(int(len(data)/2))].copy()\n",
    "# df1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = data.iloc[(int(len(data)/2)):].copy()\n",
    "# df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1['tokenized_by_sent'] = df1['comment'].apply(sent_tokenize)\n",
    "#df2['tokenized_by_sent'] = df2['comment'].apply(sent_tokenize)\n",
    "\n",
    "#data['tokenized_by_sent'] = data['comment'].apply(sent_tokenize)\n",
    "#data['tokenized_by_word'] = data['comment'].apply(word_tokenize)\n",
    "#word = word_tokenize(sarcastic['comment'].iloc[56269])\n",
    "#word\n",
    "#note to nour, so when i try to do the first line of code it shows that index 56269 has an error but when i try to \n",
    "#individually tokenized that index, it works. Not sure how to fix this.\n",
    "\n",
    "# note to Juan: Fixed by making dropna function is working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize by word first half\n",
    "\n",
    "# df1['comment'] = df1['comment'].apply(word_tokenize)\n",
    "# df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize by word second half\n",
    "\n",
    "# df2['comment'] = df2['comment'].apply(word_tokenize)\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts of Speech tagging first half\n",
    "\n",
    "# df1['POS_tag'] = df1['comment'].apply(nltk.pos_tag)\n",
    "# df1.head()\n",
    "\n",
    "#data['POS_tag'] = data['tokenized_by_word'].apply(nltk.pos_tag)\n",
    "#nltk.pos_tag(data.iloc[0, 5])\n",
    "#entities = nltk.chunk.ne_chunk( nltk.pos_tag(data.iloc[1, 5]))\n",
    "#entities.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts of Speech tagging second half\n",
    "\n",
    "# df2['POS_tag'] = df2['comment'].apply(nltk.pos_tag)\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head()\n",
    "# df1_len = len(df1)\n",
    "# df2_len = len(df2)\n",
    "# df14 = df1.iloc[0:int(df1_len/2)].copy()\n",
    "# df24 = df1.iloc[(int(df1_len/2)):].copy()\n",
    "# df34 = df2.iloc[0:int(df2_len/2)].copy()\n",
    "# df44 = df2.iloc[(int(df2_len/2)):].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "#from nltk.stem import PorterStemmer\n",
    "\n",
    "#data['stem'] = data['tokenized_by_word'].apply(lambda x: [PorterStemmer.stem(y) for y in x])\n",
    "\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# df14['stem'] = df14['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# df14.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df24['stem'] = df24['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# df24.head()\n",
    "#df2['stem'] = df2['tokenized_by_word'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "#df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df34['stem'] = df34['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# df34.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df44['stem'] = df44['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# df44.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.stem import WordNetLemmatizer\n",
    "#data['lemma'] = data['tokenized_by_word'].apply(WordNetLemmatizer)\n",
    "# df14.append(df24.append(df34.append(df44)))\n",
    "# df14.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Sarcasm\n",
    "\n",
    "We were interested in trying different models for predicting a sarcastic comment.\n",
    "To determine the features and labels for the analysis, we decided to look at the comment itself as a feature and use the given sarcastic vs non-sarcastic classification as our label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the features and labels for the models\n",
    "features = chosen_data['stem'].apply(lambda x: ' '.join(x))\n",
    "labels = chosen_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF to vectorize the data\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer() # ask about max features\n",
    "features = list(features)\n",
    "X = vectorizer.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101076, 37023)\n"
     ]
    }
   ],
   "source": [
    "#print(vectorizer.get_feature_names())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<101076x37023 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 953981 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(X)\n",
    "X.toarray() # it only works if it's an array but why does it become all 0s?\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Animal, rights, breh]</td>\n",
       "      <td>[(Animal, NNP), (rights, NNS), (breh, NN)]</td>\n",
       "      <td>[anim, right, breh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[Baja, la, applicasion, de, Xfinity, Tv, to, go]</td>\n",
       "      <td>[(Baja, NNP), (la, CC), (applicasion, NN), (de...</td>\n",
       "      <td>[baja, la, applicas, de, xfiniti, tv, to, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[Safety, turd, ,, in, the, hot, tub]</td>\n",
       "      <td>[(Safety, NNP), (turd, NN), (,, ,), (in, IN), ...</td>\n",
       "      <td>[safeti, turd, ,, in, the, hot, tub]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[Infinite, hiatus]</td>\n",
       "      <td>[(Infinite, NNP), (hiatus, NN)]</td>\n",
       "      <td>[infinit, hiatus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[Those, institutions, still, have, no, lasting...</td>\n",
       "      <td>[(Those, DT), (institutions, NNS), (still, RB)...</td>\n",
       "      <td>[those, institut, still, have, no, last, power...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0                             [Animal, rights, breh]   \n",
       "1      0   [Baja, la, applicasion, de, Xfinity, Tv, to, go]   \n",
       "2      0               [Safety, turd, ,, in, the, hot, tub]   \n",
       "3      0                                 [Infinite, hiatus]   \n",
       "4      0  [Those, institutions, still, have, no, lasting...   \n",
       "\n",
       "                                             POS_tag  \\\n",
       "0         [(Animal, NNP), (rights, NNS), (breh, NN)]   \n",
       "1  [(Baja, NNP), (la, CC), (applicasion, NN), (de...   \n",
       "2  [(Safety, NNP), (turd, NN), (,, ,), (in, IN), ...   \n",
       "3                    [(Infinite, NNP), (hiatus, NN)]   \n",
       "4  [(Those, DT), (institutions, NNS), (still, RB)...   \n",
       "\n",
       "                                                stem  \n",
       "0                                [anim, right, breh]  \n",
       "1      [baja, la, applicas, de, xfiniti, tv, to, go]  \n",
       "2               [safeti, turd, ,, in, the, hot, tub]  \n",
       "3                                  [infinit, hiatus]  \n",
       "4  [those, institut, still, have, no, last, power...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation?\n",
    "\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Test data chosen_data[['comment', 'POS_tag', 'stem']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, chosen_data.label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "log_clf = LogisticRegression(solver='lbfgs', max_iter = 200).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the test values\n",
    "\n",
    "log_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7466485283205541\n",
      "0.6775326474079937\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression score\n",
    "print(log_clf.score(X_train, y_train))\n",
    "print(log_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM model\n",
    "\n",
    "svm_clf = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8067276774672273\n",
      "0.6695191927186387\n"
     ]
    }
   ],
   "source": [
    "print(svm_clf.score(X_train, y_train))\n",
    "print(svm_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "rf_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9799282710858274\n",
      "0.6704095765730115\n"
     ]
    }
   ],
   "source": [
    "print(rf_clf.score(X_train, y_train))\n",
    "print(rf_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
