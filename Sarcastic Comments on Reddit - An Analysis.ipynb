{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nour Mansour and Juan Estrella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Data Collection\n",
    "\n",
    "Step 1: Get the tsv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0      0                                         NC and NH.\n",
       "1      0  You do know west teams play against west teams...\n",
       "2      0  They were underdogs earlier today, but since G...\n",
       "3      0  This meme isn't funny none of the \"new york ni...\n",
       "4      0                    I could use one of those tools."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data is saved in the same folder as the project. Then read data from tsv file\n",
    "data = pd.read_csv(\"train-balanced-sarcasm.csv\")\n",
    "data.dropna(inplace=True)\n",
    "data.drop(['author', 'score', 'ups', 'downs', 'date', 'created_utc', 'subreddit', 'parent_comment'], axis = 1, inplace = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Data Processing\n",
    "\n",
    "Step 1: Columns required: Label, Comments, subreddit, parent comment\n",
    "\n",
    "Step 2: Create a Dataframe containing an even amount of sarcastic and non sarcastic \n",
    "comments, amount of data is 505413 each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1010768</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm sure that Iran and N. Korea have the techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010769</th>\n",
       "      <td>1</td>\n",
       "      <td>whatever you do, don't vote green!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010770</th>\n",
       "      <td>1</td>\n",
       "      <td>Perhaps this is an atheist conspiracy to make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010771</th>\n",
       "      <td>1</td>\n",
       "      <td>The Slavs got their own country - it is called...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010772</th>\n",
       "      <td>1</td>\n",
       "      <td>values, as in capitalism .. there is good mone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                            comment\n",
       "1010768      1  I'm sure that Iran and N. Korea have the techn...\n",
       "1010769      1                 whatever you do, don't vote green!\n",
       "1010770      1  Perhaps this is an atheist conspiracy to make ...\n",
       "1010771      1  The Slavs got their own country - it is called...\n",
       "1010772      1  values, as in capitalism .. there is good mone..."
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Copy the only the data we need\n",
    "#required_data = data[['label','comment','subreddit','parent_comment']]\n",
    "#required_data.head()\n",
    "# Reset the indices after rows with NA values are dropped\n",
    "data.reset_index(inplace = True)\n",
    "data.drop(['index'], axis = 1, inplace = True)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>198103</th>\n",
       "      <td>0</td>\n",
       "      <td>Hitsugaya being in every fight is partly becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546828</th>\n",
       "      <td>0</td>\n",
       "      <td>I imagine his nickname in prison would be 'The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989843</th>\n",
       "      <td>0</td>\n",
       "      <td>what about the part where its solo queue :/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7539</th>\n",
       "      <td>0</td>\n",
       "      <td>Yea great example, I see Gold players pull off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890177</th>\n",
       "      <td>0</td>\n",
       "      <td>Well it must've been a glitch then, because I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                            comment\n",
       "0 198103      0  Hitsugaya being in every fight is partly becau...\n",
       "  546828      0  I imagine his nickname in prison would be 'The...\n",
       "  989843      0        what about the part where its solo queue :/\n",
       "  7539        0  Yea great example, I see Gold players pull off...\n",
       "  890177      0  Well it must've been a glitch then, because I ..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rand_ind = np.random.choice(data.index, int(len(data)*10/100))\n",
    "#len(rand_ind) # 101077 for 10%\n",
    "\n",
    "#chosen_data = data.sample(frac=0.5, random_state=1)\n",
    "#chosen_data.tail()\n",
    "\n",
    "\n",
    "\n",
    "size = int(len(data)*5/100)# sample size\n",
    "replace = True  # with replacement\n",
    "fn = lambda obj: obj.loc[np.random.choice(obj.index, size, replace),:]\n",
    "chosen_data = data.groupby('label', as_index=False).apply(fn)\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hitsugaya being in every fight is partly becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I imagine his nickname in prison would be 'The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>what about the part where its solo queue :/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Yea great example, I see Gold players pull off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Well it must've been a glitch then, because I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0      0  Hitsugaya being in every fight is partly becau...\n",
       "1      0  I imagine his nickname in prison would be 'The...\n",
       "2      0        what about the part where its solo queue :/\n",
       "3      0  Yea great example, I see Gold players pull off...\n",
       "4      0  Well it must've been a glitch then, because I ..."
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_data.reset_index(inplace = True)\n",
    "chosen_data.drop(['level_0', 'level_1'], inplace = True, axis = 1)\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101071</th>\n",
       "      <td>1</td>\n",
       "      <td>Number 13 will blow your mind!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101072</th>\n",
       "      <td>1</td>\n",
       "      <td>Surprised to hear them featured in the NME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101073</th>\n",
       "      <td>1</td>\n",
       "      <td>Stay classy reddit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101074</th>\n",
       "      <td>1</td>\n",
       "      <td>Random Beta invites for viewers would be a start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101075</th>\n",
       "      <td>1</td>\n",
       "      <td>the new robbie schremp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                           comment\n",
       "101071      1                    Number 13 will blow your mind!\n",
       "101072      1        Surprised to hear them featured in the NME\n",
       "101073      1                               Stay classy reddit.\n",
       "101074      1  Random Beta invites for viewers would be a start\n",
       "101075      1                            the new robbie schremp"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Exploratory Analysis & Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHMZJREFUeJzt3X28XVV95/HP10QeFEEeAoUECRba8jBVS6RUbavFEdRadAY0ViVVpkyttVqtCtZWHcsUZlr1hR20jFgCtkJEHVDBiiBqLQaDighKiaAQQ0l4FFTQ4G/+2Ovqyc19OAn73Msln/frdV5nn7X3Wmetc+8937P2PnfvVBWSJPXhEbPdAUnSw4ehIknqjaEiSeqNoSJJ6o2hIknqjaEiSeqNoaJNJHlfkr/sqa3HJbk3ybz2+LIk/62Ptlt7FyVZ1ld7m/G8f53ktiT/MdPPPRclWZykksyfxT78QZJ/nem6WxtDZSuT5DtJfpTkniR3Jfm3JH+U5Ge/C1X1R1X1jiHbeuZU21TVTVW1Q1U90EPf35bkg+Paf3ZVLX+wbW9mP/YGXg8cWFW/MMk2OyZ5d5KbWqiubo93m8m+9umhEAytH5v8Huihw1DZOj2vqh4D7AOcDLwJOKPvJ5ntN58R2ge4varWTbQyyTbAJcBBwJHAjsBTgNuBQ2eqk9JsMFS2YlV1d1VdALwIWJbkYIAkZyb567a8W5JPtFnNHUm+kOQRSc4GHgd8vH0Sf+PAJ9njktwEXDrJp9tfTHJFkruTnJ9kl/ZcT0+yZrCPY7OhJEcCbwZe1J7vqrb+Z7vTWr/ekuS7SdYlOSvJTm3dWD+WtdnDbUn+YrLXJslOrf761t5bWvvPBC4G9mr9OHOC6se21+YFVXVtVf20qtZV1Tuq6sLW/gGt73cluSbJ7w0895lJTmu79u5N8sUkv9BmOncm+VaSJ417jd6Q5OtJfpDkjCR7tPr3JPlMkp0Htj+szVDvSnJVkqcPrLssyTvac96T5NMDs6vPt/u7Wr9+I8l+ST7Xfpa3JTl3ste0eUWStUluSfL69py/kOSHSXYd6Mch7bV/5DTtbSTJCUm+3fp+bZIXbLpJ3tP6+60khw+s2Km9drck+V66XZzzNuf5ZagIqKorgDXAb06w+vVt3QJgD7o39qqqlwE30c16dqiq/zVQ57eBA4AjJnnKY4FXAHsBG4BTh+jjp4D/CZzbnu8JE2z2B+32DODxwA7A34/b5mnALwOHA3+V5IBJnvI9wE6tnd9ufX55VX0GeDawtvXjDyao+0zgU1V170QNtzfKjwOfBnYHXg38U5JfHtjshcBbgN2A+4HLga+0x+cB7xzX7H8F/jPwS8DzgIvofla70f2d/2l77oXAJ4G/BnYB/hz4SJIFA239PvDy1rdt2jYAv9XuH9vGfjnwjjaOnYFF7XWbyjOA/YFnASckeWZV/QdwWRvzmJcC51TVT6Zpb7xv0/0e7wS8Hfhgkj0H1v86cAPd6/JW4KNjH2qA5XS/j/sBT2p97O3439bCUNGYtXRvMuP9BNgT2KeqflJVX6jpTxj3tqr6QVX9aJL1Z1fVN6rqB8BfAi/s6RPhS4B3VtUN7Q39RGDpuFnS26vqR1V1FXAVsEk4tb68CDixqu6pqu8Afwe8bMh+7ArcMsX6w+gC7+Sq+nFVXQp8AnjxwDYfq6orq+o+4GPAfVV1Vjs2dS7dm96g91TVrVX1PeALwMqq+mpV3d/qj23/UuDCqrqwzaAuBlYBzxlo6x+r6t/bz28F8MQpxvITut2Be1XVfVU13cHst7ffjauBfxwY8/LWt7HX/8XA2dO0tYmq+nBVrW1jOxe4no13Oa4D3t1+l88FrgOem2QPug8Lr239Wwe8C1i6uX3Y2hkqGrMQuGOC8v8NrAY+neSGJCcM0dbNm7H+u8Aj6T45Plh7tfYG255PN8MaM/htrR/SvbmPtxvdJ/TxbS0csh+30wXxVP28uap+OkX7tw4s/2iCx+P7Pez2+wDHtF1fdyW5i272NtjfYV6jMW8EAlzRduO9YoptYdOf/V5t+XzgwCSPp5tx3d1m0JslybFJvjYwtoPZ+Hfre+M+FI31YR+638NbBur+A91sTZvBUBFJnkz3hrbJp8z2Sf31VfV4ut0qrxvYDz3ZjGW6mczeA8uPo/u0exvwA+BRA/2aR7fbbdh219K9OQy2vYGN32CHcRs//wQ+2Nb3hqz/GeCIJI+eop97Z+Abd5vZ/oNxM91M8bEDt0dX1clD1N3k9a+q/6iqP6yqvYD/DpyWZL8p2hj/s1/b2rmPblb0EroZ4WbPUpLsA/xf4E+AXavqscA36EJvzMIkg4/H+nAz3W7G3QZelx2r6qDN7cfWzlDZiqX72uvvAucAH2y7JMZv87vtYGyA7wMPtBt0b9aP34KnfmmSA5M8CvgfwHltt86/A9sleW477vAWYNuBercCi8e9GQ/6EPBnSfZNsgM/PwazYXM61/qyAjgpyWPam9XrgGG/xno23ZvUR5L8SjvAv2uSNyd5DrCSLkDfmOSR7UD58+h+DqP2QeB5SY5IMi/Jdum+ILFoiLrrgZ8y8DNPcsxA3Tvpgmeqr4//ZZJHJTmI7rjN4IH9s+iOif0e07/Wj2h9H7ttCzy6Pf/61reX081UBu0O/Gl73Y+hO/Z3YVXdQnds6O/a38Ujkvxikt+eph8ax1DZOn08yT10b3x/QXfQ9+WTbLs/3Sfve+kOFp9WVZe1dX8DvKXtLvjzSepP5GzgTLrdLNvRDiJX1d3AHwPvp/vU/gO6LwmM+XC7vz3JVyZo9wOt7c8DNwL30R0E3xKvbs9/A90M7p9b+9NqxzGeCXyL7pti3weuoNsNs7Kqfkz3xvlsulnRacCxVfWtLezr0KrqZuAouoP46+l+B97AEO8FVfVD4CTgi+1nfhjwZGBlknuBC4DXVNWNUzTzObrdqZcAf1tVnx5o/4t0ofWVdhxrKi+m2603dvt2VV1Ld+zrcroPIP8J+OK4eivpfqdva2M5uqpub+uOpdvteS1dQJ7H1LsxNYF4kS5JDxVJLgX+uareP9t90ZYxVCQ9JLRjexcDe1fVPbPdH20Zd39JmnVJltPtZn2tgTK3OVORJPXGmYokqTcP1xP+TWq33XarxYsXz3Y3JGlOufLKK2+rqgXTbbfVhcrixYtZtWrVbHdDkuaUJN+dfit3f0mSemSoSJJ6Y6hIknpjqEiSemOoSJJ6Y6hIknpjqEiSemOoSJJ6M9JQSfKdJFe3y3uuamW7JLk4yfXtfueB7U9MsjrJdUmOGCg/pLWzOsmpY1duS7JtknNb+coki0c5HknS1GbiP+qfUVW3DTw+Abikqk5u1zs/AXhTkgOBpcBBdNeM/kySX2pX4XsvcDzwJeBC4EjgIuA44M6q2i/JUuAU4EWjGsjiEz45qqan9Z2Tnztrzy2pPw/395HZ2P11FLC8LS8Hnj9Qfk5V3d+uHLcaODTJnsCOVXV5dadUPmtcnbG2zgMOH3f9aUnSDBp1qBTw6SRXJjm+le3RrgdNu9+9lS+ku7TpmDWtbCEbX1J2rHyjOu065HcDu47vRJLjk6xKsmr9+vW9DEyStKlR7/56alWtTbI7cHGSqa7BPdEMo6Yon6rOxgVVpwOnAyxZssQLyEjSiIx0plJVa9v9OuBjwKHArW2XFu1+Xdt8DbD3QPVFwNpWvmiC8o3qJJkP7ATcMYqxSJKmN7JQSfLoJI8ZWwaeBXwDuABY1jZbBpzfli8AlrZvdO0L7A9c0XaR3ZPksHa85NhxdcbaOhq4tLyUpSTNmlHu/toD+Fg7bj4f+Oeq+lSSLwMrkhwH3AQcA1BV1yRZAVwLbABe1b75BfBK4Exge7pvfV3Uys8Azk6ymm6GsnSE45EkTWNkoVJVNwBPmKD8duDwSeqcBJw0Qfkq4OAJyu+jhZIkafb5H/WSpN4YKpKk3hgqkqTeGCqSpN4YKpKk3hgqkqTeGCqSpN4YKpKk3hgqkqTeGCqSpN4YKpKk3hgqkqTeGCqSpN4YKpKk3hgqkqTeGCqSpN4YKpKk3hgqkqTeGCqSpN4YKpKk3hgqkqTeGCqSpN4YKpKk3hgqkqTeGCqSpN4YKpKk3hgqkqTeGCqSpN4YKpKk3hgqkqTeGCqSpN4YKpKk3ow8VJLMS/LVJJ9oj3dJcnGS69v9zgPbnphkdZLrkhwxUH5IkqvbulOTpJVvm+TcVr4yyeJRj0eSNLmZmKm8BvjmwOMTgEuqan/gkvaYJAcCS4GDgCOB05LMa3XeCxwP7N9uR7by44A7q2o/4F3AKaMdiiRpKiMNlSSLgOcC7x8oPgpY3paXA88fKD+nqu6vqhuB1cChSfYEdqyqy6uqgLPG1Rlr6zzg8LFZjCRp5o16pvJu4I3ATwfK9qiqWwDa/e6tfCFw88B2a1rZwrY8vnyjOlW1Abgb2HV8J5Icn2RVklXr169/sGOSJE1iZKGS5HeBdVV15bBVJiirKcqnqrNxQdXpVbWkqpYsWLBgyO5IkjbX/BG2/VTg95I8B9gO2DHJB4Fbk+xZVbe0XVvr2vZrgL0H6i8C1rbyRROUD9ZZk2Q+sBNwx6gGJEma2shmKlV1YlUtqqrFdAfgL62qlwIXAMvaZsuA89vyBcDS9o2ufekOyF/RdpHdk+Swdrzk2HF1xto6uj3HJjMVSdLMGOVMZTInAyuSHAfcBBwDUFXXJFkBXAtsAF5VVQ+0Oq8EzgS2By5qN4AzgLOTrKaboSydqUFIkjY1I6FSVZcBl7Xl24HDJ9nuJOCkCcpXAQdPUH4fLZQkSbPP/6iXJPXGUJEk9cZQkST1xlCRJPXGUJEk9cZQkST1xlCRJPXGUJEk9cZQkST1xlCRJPXGUJEk9cZQkST1xlCRJPXGUJEk9cZQkST1xlCRJPXGUJEk9cZQkST1xlCRJPXGUJEk9cZQkST1xlCRJPXGUJEk9cZQkST1xlCRJPXGUJEk9cZQkST1xlCRJPXGUJEk9cZQkST1xlCRJPXGUJEk9WZkoZJkuyRXJLkqyTVJ3t7Kd0lycZLr2/3OA3VOTLI6yXVJjhgoPyTJ1W3dqUnSyrdNcm4rX5lk8ajGI0ma3ihnKvcDv1NVTwCeCByZ5DDgBOCSqtofuKQ9JsmBwFLgIOBI4LQk81pb7wWOB/ZvtyNb+XHAnVW1H/Au4JQRjkeSNI2RhUp17m0PH9luBRwFLG/ly4Hnt+WjgHOq6v6quhFYDRyaZE9gx6q6vKoKOGtcnbG2zgMOH5vFSJJm3kiPqSSZl+RrwDrg4qpaCexRVbcAtPvd2+YLgZsHqq9pZQvb8vjyjepU1QbgbmDXCfpxfJJVSVatX7++r+FJksYZaahU1QNV9URgEd2s4+ApNp9ohlFTlE9VZ3w/Tq+qJVW1ZMGCBdN1W5K0hYYKlSRPHaZsMlV1F3AZ3bGQW9suLdr9urbZGmDvgWqLgLWtfNEE5RvVSTIf2Am4Y9h+SZL6NexM5T1Dlv1MkgVJHtuWtweeCXwLuABY1jZbBpzfli8AlrZvdO1Ld0D+iraL7J4kh7XjJceOqzPW1tHApe24iyRpFsyfamWS3wCeAixI8rqBVTsC8yau9TN7AsvbN7geAayoqk8kuRxYkeQ44CbgGICquibJCuBaYAPwqqp6oLX1SuBMYHvgonYDOAM4O8lquhnK0umHLEkalSlDBdgG2KFt95iB8u/TzQwmVVVfB540QfntwOGT1DkJOGmC8lXAJsdjquo+WihJkmbflKFSVZ8DPpfkzKr67gz1SZI0R003UxmzbZLTgcWDdarqd0bRKUnS3DRsqHwYeB/wfuCBabaVJG2lhg2VDVX13pH2RJI05w37leKPJ/njJHu2E0LukmSXkfZMkjTnDDtTGftfkDcMlBXw+H67I0may4YKlarad9QdkSTNfUOFSpJjJyqvqrP67Y4kaS4bdvfXkweWt6P758Wv0J2GXpIkYPjdX68efJxkJ+DskfRIkjRnbemp739Id8JHSZJ+ZthjKh/n59cpmQccAKwYVackSXPTsMdU/nZgeQPw3apaM9nGkqSt01C7v9qJJb9Fd6binYEfj7JTkqS5adgrP74QuILuNPMvBFYmmfLU95Kkrc+wu7/+AnhyVa2D7qqOwGeA80bVMUnS3DPst78eMRYoze2bUVeStJUYdqbyqST/AnyoPX4RcOFouiRJmqumu0b9fsAeVfWGJP8FeBoQ4HLgn2agf5KkOWS6XVjvBu4BqKqPVtXrqurP6GYp7x515yRJc8t0obK4qr4+vrCqVtFdWliSpJ+ZLlS2m2Ld9n12RJI0900XKl9O8ofjC5McB1w5mi5Jkuaq6b799VrgY0lews9DZAmwDfCCUXZMkjT3TBkqVXUr8JQkzwAObsWfrKpLR94zSdKcM+z1VD4LfHbEfZEkzXH+V7wkqTeGiiSpN4aKJKk3hookqTeGiiSpN4aKJKk3IwuVJHsn+WySbya5JslrWvkuSS5Ocn2733mgzolJVie5LskRA+WHJLm6rTs1SVr5tknObeUrkywe1XgkSdMb5UxlA/D6qjoAOAx4VZIDgROAS6pqf+CS9pi2bilwEHAkcFqSea2t9wLHA/u325Gt/DjgzqraD3gXcMoIxyNJmsbIQqWqbqmqr7Tle4BvAguBo4DlbbPlwPPb8lHAOVV1f1XdCKwGDk2yJ7BjVV1eVQWcNa7OWFvnAYePzWIkSTNvRo6ptN1STwJW0l306xboggfYvW22ELh5oNqaVrawLY8v36hOVW0A7gZ2neD5j0+yKsmq9evX9zMoSdImRh4qSXYAPgK8tqq+P9WmE5TVFOVT1dm4oOr0qlpSVUsWLFgwXZclSVtopKGS5JF0gfJPVfXRVnxr26VFu1/XytcAew9UXwSsbeWLJijfqE6S+cBOwB39j0SSNIxRfvsrwBnAN6vqnQOrLgCWteVlwPkD5UvbN7r2pTsgf0XbRXZPksNam8eOqzPW1tHApe24iyRpFgx1luIt9FTgZcDVSb7Wyt4MnAysaBf6ugk4BqCqrkmyAriW7ptjr6qqB1q9VwJn0l1t8qJ2gy60zk6ymm6GsnSE45EkTWNkoVJV/8rExzwADp+kzknASROUr+Ln13MZLL+PFkqSpNnnf9RLknpjqEiSemOoSJJ6Y6hIknpjqEiSemOoSJJ6Y6hIknpjqEiSemOoSJJ6Y6hIknpjqEiSemOoSJJ6Y6hIknpjqEiSemOoSJJ6Y6hIknpjqEiSemOoSJJ6Y6hIknpjqEiSemOoSJJ6Y6hIknpjqEiSemOoSJJ6Y6hIknpjqEiSemOoSJJ6Y6hIknpjqEiSemOoSJJ6Y6hIknpjqEiSejOyUEnygSTrknxjoGyXJBcnub7d7zyw7sQkq5Ncl+SIgfJDklzd1p2aJK182yTntvKVSRaPaiySpOGMcqZyJnDkuLITgEuqan/gkvaYJAcCS4GDWp3Tksxrdd4LHA/s325jbR4H3FlV+wHvAk4Z2UgkSUMZWahU1eeBO8YVHwUsb8vLgecPlJ9TVfdX1Y3AauDQJHsCO1bV5VVVwFnj6oy1dR5w+NgsRpI0O2b6mMoeVXULQLvfvZUvBG4e2G5NK1vYlseXb1SnqjYAdwO7TvSkSY5PsirJqvXr1/c0FEnSeA+VA/UTzTBqivKp6mxaWHV6VS2pqiULFizYwi5KkqYz06Fya9ulRbtf18rXAHsPbLcIWNvKF01QvlGdJPOBndh0d5skaQbNdKhcACxry8uA8wfKl7ZvdO1Ld0D+iraL7J4kh7XjJceOqzPW1tHApe24iyRplswfVcNJPgQ8HdgtyRrgrcDJwIokxwE3AccAVNU1SVYA1wIbgFdV1QOtqVfSfZNse+CidgM4Azg7yWq6GcrSUY1FkjSckYVKVb14klWHT7L9ScBJE5SvAg6eoPw+WihJkh4aHioH6iVJDwOGiiSpN4aKJKk3hookqTeGiiSpN4aKJKk3hookqTeGiiSpN4aKJKk3hookqTeGiiSpN4aKJKk3hookqTeGiiSpN4aKJKk3hookqTeGiiSpN4aKJKk3hookqTeGiiSpN4aKJKk3hookqTeGiiSpN4aKJKk3hookqTeGiiSpN4aKJKk3hookqTeGiiSpN4aKJKk3hookqTeGiiSpN3M+VJIcmeS6JKuTnDDb/ZGkrdmcDpUk84D/AzwbOBB4cZIDZ7dXkrT1mtOhAhwKrK6qG6rqx8A5wFGz3CdJ2mrNn+0OPEgLgZsHHq8Bfn38RkmOB45vD+9Nct0WPt9uwG1bWPdBySmz8azALI55FjnmrcNWN+ac8qDGvM8wG831UMkEZbVJQdXpwOkP+smSVVW15MG2M5c45q2DY946zMSY5/rurzXA3gOPFwFrZ6kvkrTVm+uh8mVg/yT7JtkGWApcMMt9kqSt1pze/VVVG5L8CfAvwDzgA1V1zQif8kHvQpuDHPPWwTFvHUY+5lRtcghCkqQtMtd3f0mSHkIMFUlSbwyVCUx36pd0Tm3rv57k12ajn30aYswvaWP9epJ/S/KE2ehnn4Y9xU+SJyd5IMnRM9m/URhmzEmenuRrSa5J8rmZ7mOfhvi93inJx5Nc1cb78tnoZ5+SfCDJuiTfmGT9aN+/qsrbwI3ugP+3gccD2wBXAQeO2+Y5wEV0/ydzGLBytvs9A2N+CrBzW3721jDmge0uBS4Ejp7tfs/Az/mxwLXA49rj3We73yMe75uBU9ryAuAOYJvZ7vuDHPdvAb8GfGOS9SN9/3KmsqlhTv1yFHBWdb4EPDbJnjPd0R5NO+aq+requrM9/BLd/wTNZcOe4ufVwEeAdTPZuREZZsy/D3y0qm4CqKq5PO5hxlvAY5IE2IEuVDbMbDf7VVWfpxvHZEb6/mWobGqiU78s3IJt5pLNHc9xdJ905rJpx5xkIfAC4H0z2K9RGubn/EvAzkkuS3JlkmNnrHf9G2a8fw8cQPdP01cDr6mqn85M92bNSN+/5vT/qYzIMKd+Ger0MHPI0ONJ8gy6UHnaSHs0esOM+d3Am6rqge6D7Jw3zJjnA4cAhwPbA5cn+VJV/fuoOzcCw4z3COBrwO8AvwhcnOQLVfX9UXduFo30/ctQ2dQwp355uJ0eZqjxJPlV4P3As6vq9hnq26gMM+YlwDktUHYDnpNkQ1X9v5npYu+G/d2+rap+APwgyeeBJwBzMVSGGe/LgZOrO9iwOsmNwK8AV8xMF2fFSN+/3P21qWFO/XIBcGz7FsVhwN1VdctMd7RH0445yeOAjwIvm6OfWsebdsxVtW9VLa6qxcB5wB/P4UCB4X63zwd+M8n8JI+iO+v3N2e4n30ZZrw30c3KSLIH8MvADTPay5k30vcvZyrj1CSnfknyR239++i+CfQcYDXwQ7pPO3PWkGP+K2BX4LT2yX1DzeEzvA455oeVYcZcVd9M8ing68BPgfdX1YRfTX2oG/Jn/A7gzCRX0+0WelNVzenT4Sf5EPB0YLcka4C3Ao+EmXn/8jQtkqTeuPtLktQbQ0WS1BtDRZLUG0NFktQbQ0WS1BtDRRqRJPduxrZvS/Lno2pfmimGiiSpN4aKNIOSPC/JyiRfTfKZ9l/cY56Q5NIk1yf5w4E6b0jy5Xbti7fPQreloRkq0sz6V+CwqnoS3anY3ziw7leB5wK/AfxVkr2SPAvYn+407k8EDknyWzPcZ2lonqZFmlmLgHPb9Su2AW4cWHd+Vf0I+FGSz9IFydOAZwFfbdvsQBcyn5+5LkvDM1SkmfUe4J1VdUGSpwNvG1g3/pxJRXc+qr+pqn+Yme5JD467v6SZtRPwvba8bNy6o5Jsl2RXuhMCfpnuZIivSLIDdBcOS7L7THVW2lzOVKTReVQ7S+yYd9LNTD6c5Ht0l2Xed2D9FcAngccB76iqtcDaJAfQXSwL4F7gpTw8Lm+shyHPUixJ6o27vyRJvTFUJEm9MVQkSb0xVCRJvTFUJEm9MVQkSb0xVCRJvfn/mx4w7+1i+hIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore distribution of the data by label (0 -> non-sarcastic, 1 -> sarcastic)\n",
    "plt.hist(chosen_data.label)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Comments by Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">comment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50538</td>\n",
       "      <td>46926</td>\n",
       "      <td>Yes</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50538</td>\n",
       "      <td>47165</td>\n",
       "      <td>You forgot the</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment                            \n",
       "        count unique             top freq\n",
       "label                                    \n",
       "0       50538  46926             Yes   38\n",
       "1       50538  47165  You forgot the  138"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore distribution of comments by label \n",
    "chosen_data.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.4.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "#Import NLTK library\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #Use this to get rid off meaningless words like \"the, and, a\"\n",
    "from nltk.tokenize import word_tokenize #Split by word\n",
    "from nltk.tokenize import sent_tokenize #Split by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure all the comment column is str data type\n",
    "chosen_data['comment'] = chosen_data['comment'].astype(str)\n",
    "chosen_data['comment'] = chosen_data['comment'].astype(str)\n",
    "\n",
    "#non_sarcastic = required_data.loc[required_data['label'] == 0]\n",
    "#sarcastic = required_data.loc[required_data['label'] == 1]\n",
    "\n",
    "#drop rows with na values on the comment column\n",
    "#non_sarcastic['comment'].dropna(inplace=True)\n",
    "#sarcastic['comment'].dropna(inplace=True)\n",
    "\n",
    "#Make sure all the comment column is str data type\n",
    "#non_sarcastic['comment'] = non_sarcastic['comment'].astype(str)\n",
    "#sarcastic['comment'] = sarcastic['comment'].astype(str)\n",
    "#print(len(non_sarcastic), len(sarcastic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>POS_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Hitsugaya, being, in, every, fight, is, partl...</td>\n",
       "      <td>[(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[I, imagine, his, nickname, in, prison, would,...</td>\n",
       "      <td>[(I, PRP), (imagine, VBP), (his, PRP$), (nickn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[what, about, the, part, where, its, solo, que...</td>\n",
       "      <td>[(what, WP), (about, IN), (the, DT), (part, NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[Yea, great, example, ,, I, see, Gold, players...</td>\n",
       "      <td>[(Yea, NNP), (great, JJ), (example, NN), (,, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[Well, it, must, 've, been, a, glitch, then, ,...</td>\n",
       "      <td>[(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  [Hitsugaya, being, in, every, fight, is, partl...   \n",
       "1      0  [I, imagine, his, nickname, in, prison, would,...   \n",
       "2      0  [what, about, the, part, where, its, solo, que...   \n",
       "3      0  [Yea, great, example, ,, I, see, Gold, players...   \n",
       "4      0  [Well, it, must, 've, been, a, glitch, then, ,...   \n",
       "\n",
       "                                             POS_tag  \n",
       "0  [(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...  \n",
       "1  [(I, PRP), (imagine, VBP), (his, PRP$), (nickn...  \n",
       "2  [(what, WP), (about, IN), (the, DT), (part, NN...  \n",
       "3  [(Yea, NNP), (great, JJ), (example, NN), (,, ,...  \n",
       "4  [(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_data['comment'] = chosen_data['comment'].apply(word_tokenize)\n",
    "chosen_data['POS_tag'] = chosen_data['comment'].apply(nltk.pos_tag)\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Hitsugaya, being, in, every, fight, is, partl...</td>\n",
       "      <td>[(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...</td>\n",
       "      <td>[hitsugaya, be, in, everi, fight, is, part, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[I, imagine, his, nickname, in, prison, would,...</td>\n",
       "      <td>[(I, PRP), (imagine, VBP), (his, PRP$), (nickn...</td>\n",
       "      <td>[i, imagin, his, nicknam, in, prison, would, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[what, about, the, part, where, its, solo, que...</td>\n",
       "      <td>[(what, WP), (about, IN), (the, DT), (part, NN...</td>\n",
       "      <td>[what, about, the, part, where, it, solo, queu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[Yea, great, example, ,, I, see, Gold, players...</td>\n",
       "      <td>[(Yea, NNP), (great, JJ), (example, NN), (,, ,...</td>\n",
       "      <td>[yea, great, exampl, ,, i, see, gold, player, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[Well, it, must, 've, been, a, glitch, then, ,...</td>\n",
       "      <td>[(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...</td>\n",
       "      <td>[well, it, must, ve, been, a, glitch, then, ,,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  [Hitsugaya, being, in, every, fight, is, partl...   \n",
       "1      0  [I, imagine, his, nickname, in, prison, would,...   \n",
       "2      0  [what, about, the, part, where, its, solo, que...   \n",
       "3      0  [Yea, great, example, ,, I, see, Gold, players...   \n",
       "4      0  [Well, it, must, 've, been, a, glitch, then, ,...   \n",
       "\n",
       "                                             POS_tag  \\\n",
       "0  [(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...   \n",
       "1  [(I, PRP), (imagine, VBP), (his, PRP$), (nickn...   \n",
       "2  [(what, WP), (about, IN), (the, DT), (part, NN...   \n",
       "3  [(Yea, NNP), (great, JJ), (example, NN), (,, ,...   \n",
       "4  [(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...   \n",
       "\n",
       "                                                stem  \n",
       "0  [hitsugaya, be, in, everi, fight, is, part, be...  \n",
       "1  [i, imagin, his, nicknam, in, prison, would, b...  \n",
       "2  [what, about, the, part, where, it, solo, queu...  \n",
       "3  [yea, great, exampl, ,, i, see, gold, player, ...  \n",
       "4  [well, it, must, ve, been, a, glitch, then, ,,...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "chosen_data['stem'] = chosen_data['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_len = len(data)\n",
    "#data1 = data.iloc[0:(int(data_len/4))].copy()\n",
    "#data1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data2 = data.iloc[(int(data_len/4)):(int(data_len/2))].copy()\n",
    "#data2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data3 = data.iloc[(int(data_len/2)):(int(3*data_len/4))].copy()\n",
    "#data3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data4 = data.iloc[(int(3*data_len/4)):].copy()\n",
    "#data4.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data4['comment'] = data4['comment'].apply(word_tokenize)\n",
    "# data4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data4['POS_tag'] = data4['comment'].apply(nltk.pos_tag)\n",
    "# data4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data4['stem'] = data4['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# data4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data3['comment'] = data3['comment'].apply(word_tokenize)\n",
    "# data3['POS_tag'] = data3['comment'].apply(nltk.pos_tag)\n",
    "# data3['stem'] = data3['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2['comment'] = data2['comment'].apply(word_tokenize)\n",
    "# data2['POS_tag'] = data2['comment'].apply(nltk.pos_tag)\n",
    "# data2['stem'] = data2['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1['comment'] = data1['comment'].apply(word_tokenize)\n",
    "# data1['POS_tag'] = data1['comment'].apply(nltk.pos_tag)\n",
    "# data1['stem'] = data1['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the required data set in half to ease later comment transformations\n",
    "# df1 = data.iloc[0:(int(len(data)/2))].copy()\n",
    "# df1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = data.iloc[(int(len(data)/2)):].copy()\n",
    "# df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1['tokenized_by_sent'] = df1['comment'].apply(sent_tokenize)\n",
    "#df2['tokenized_by_sent'] = df2['comment'].apply(sent_tokenize)\n",
    "\n",
    "#data['tokenized_by_sent'] = data['comment'].apply(sent_tokenize)\n",
    "#data['tokenized_by_word'] = data['comment'].apply(word_tokenize)\n",
    "#word = word_tokenize(sarcastic['comment'].iloc[56269])\n",
    "#word\n",
    "#note to nour, so when i try to do the first line of code it shows that index 56269 has an error but when i try to \n",
    "#individually tokenized that index, it works. Not sure how to fix this.\n",
    "\n",
    "# note to Juan: Fixed by making dropna function is working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize by word first half\n",
    "\n",
    "# df1['comment'] = df1['comment'].apply(word_tokenize)\n",
    "# df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize by word second half\n",
    "\n",
    "# df2['comment'] = df2['comment'].apply(word_tokenize)\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts of Speech tagging first half\n",
    "\n",
    "# df1['POS_tag'] = df1['comment'].apply(nltk.pos_tag)\n",
    "# df1.head()\n",
    "\n",
    "#data['POS_tag'] = data['tokenized_by_word'].apply(nltk.pos_tag)\n",
    "#nltk.pos_tag(data.iloc[0, 5])\n",
    "#entities = nltk.chunk.ne_chunk( nltk.pos_tag(data.iloc[1, 5]))\n",
    "#entities.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts of Speech tagging second half\n",
    "\n",
    "# df2['POS_tag'] = df2['comment'].apply(nltk.pos_tag)\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head()\n",
    "# df1_len = len(df1)\n",
    "# df2_len = len(df2)\n",
    "# df14 = df1.iloc[0:int(df1_len/2)].copy()\n",
    "# df24 = df1.iloc[(int(df1_len/2)):].copy()\n",
    "# df34 = df2.iloc[0:int(df2_len/2)].copy()\n",
    "# df44 = df2.iloc[(int(df2_len/2)):].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "#from nltk.stem import PorterStemmer\n",
    "\n",
    "#data['stem'] = data['tokenized_by_word'].apply(lambda x: [PorterStemmer.stem(y) for y in x])\n",
    "\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# df14['stem'] = df14['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# df14.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df24['stem'] = df24['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# df24.head()\n",
    "#df2['stem'] = df2['tokenized_by_word'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "#df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df34['stem'] = df34['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# df34.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df44['stem'] = df44['comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "# df44.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.stem import WordNetLemmatizer\n",
    "#data['lemma'] = data['tokenized_by_word'].apply(WordNetLemmatizer)\n",
    "# df14.append(df24.append(df34.append(df44)))\n",
    "# df14.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Sarcasm\n",
    "\n",
    "We were interested in trying different models for predicting a sarcastic comment.\n",
    "To determine the features and labels for the analysis, we decided to look at the comment itself as a feature and use the given sarcastic vs non-sarcastic classification as our label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the features and labels for the models\n",
    "features = chosen_data['stem'].apply(lambda x: ' '.join(x))\n",
    "labels = chosen_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF to vectorize the data\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer() # ask about max features\n",
    "features = list(features)\n",
    "X = vectorizer.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101076, 36655)\n"
     ]
    }
   ],
   "source": [
    "#print(vectorizer.get_feature_names())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<101076x36655 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 951727 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(X)\n",
    "X.toarray() # it only works if it's an array but why does it become all 0s?\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Hitsugaya, being, in, every, fight, is, partl...</td>\n",
       "      <td>[(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...</td>\n",
       "      <td>[hitsugaya, be, in, everi, fight, is, part, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[I, imagine, his, nickname, in, prison, would,...</td>\n",
       "      <td>[(I, PRP), (imagine, VBP), (his, PRP$), (nickn...</td>\n",
       "      <td>[i, imagin, his, nicknam, in, prison, would, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[what, about, the, part, where, its, solo, que...</td>\n",
       "      <td>[(what, WP), (about, IN), (the, DT), (part, NN...</td>\n",
       "      <td>[what, about, the, part, where, it, solo, queu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[Yea, great, example, ,, I, see, Gold, players...</td>\n",
       "      <td>[(Yea, NNP), (great, JJ), (example, NN), (,, ,...</td>\n",
       "      <td>[yea, great, exampl, ,, i, see, gold, player, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[Well, it, must, 've, been, a, glitch, then, ,...</td>\n",
       "      <td>[(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...</td>\n",
       "      <td>[well, it, must, ve, been, a, glitch, then, ,,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  [Hitsugaya, being, in, every, fight, is, partl...   \n",
       "1      0  [I, imagine, his, nickname, in, prison, would,...   \n",
       "2      0  [what, about, the, part, where, its, solo, que...   \n",
       "3      0  [Yea, great, example, ,, I, see, Gold, players...   \n",
       "4      0  [Well, it, must, 've, been, a, glitch, then, ,...   \n",
       "\n",
       "                                             POS_tag  \\\n",
       "0  [(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...   \n",
       "1  [(I, PRP), (imagine, VBP), (his, PRP$), (nickn...   \n",
       "2  [(what, WP), (about, IN), (the, DT), (part, NN...   \n",
       "3  [(Yea, NNP), (great, JJ), (example, NN), (,, ,...   \n",
       "4  [(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...   \n",
       "\n",
       "                                                stem  \n",
       "0  [hitsugaya, be, in, everi, fight, is, part, be...  \n",
       "1  [i, imagin, his, nicknam, in, prison, would, b...  \n",
       "2  [what, about, the, part, where, it, solo, queu...  \n",
       "3  [yea, great, exampl, ,, i, see, gold, player, ...  \n",
       "4  [well, it, must, ve, been, a, glitch, then, ,,...  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation?\n",
    "\n",
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Test data chosen_data[['comment', 'POS_tag', 'stem']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, chosen_data.label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "log_clf = LogisticRegression(solver='lbfgs', max_iter = 200).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the test values\n",
    "\n",
    "log_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7479099678456592\n",
      "0.6768401266323704\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression score\n",
    "print(log_clf.score(X_train, y_train))\n",
    "print(log_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM model\n",
    "\n",
    "svm_clf = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.807308929013109\n",
      "0.6664523149980214\n"
     ]
    }
   ],
   "source": [
    "print(svm_clf.score(X_train, y_train))\n",
    "print(svm_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "rf_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9792233489982686\n",
      "0.6721408785120696\n"
     ]
    }
   ],
   "source": [
    "print(rf_clf.score(X_train, y_train))\n",
    "print(rf_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_chunking = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunk_parser = nltk.RegexpParser(np_chunking)\n",
    "chosen_data['noun_phrase_chunk'] = chosen_data['POS_tag'].apply(chunk_parser.parse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>stem</th>\n",
       "      <th>noun_phrase_chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Hitsugaya, being, in, every, fight, is, partl...</td>\n",
       "      <td>[(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...</td>\n",
       "      <td>[hitsugaya, be, in, everi, fight, is, part, be...</td>\n",
       "      <td>[(Hitsugaya, NNP), (being, VBG), (in, IN), [(e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[I, imagine, his, nickname, in, prison, would,...</td>\n",
       "      <td>[(I, PRP), (imagine, VBP), (his, PRP$), (nickn...</td>\n",
       "      <td>[i, imagin, his, nicknam, in, prison, would, b...</td>\n",
       "      <td>[(I, PRP), (imagine, VBP), (his, PRP$), [(nick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[what, about, the, part, where, its, solo, que...</td>\n",
       "      <td>[(what, WP), (about, IN), (the, DT), (part, NN...</td>\n",
       "      <td>[what, about, the, part, where, it, solo, queu...</td>\n",
       "      <td>[(what, WP), (about, IN), [(the, DT), (part, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[Yea, great, example, ,, I, see, Gold, players...</td>\n",
       "      <td>[(Yea, NNP), (great, JJ), (example, NN), (,, ,...</td>\n",
       "      <td>[yea, great, exampl, ,, i, see, gold, player, ...</td>\n",
       "      <td>[(Yea, NNP), [(great, JJ), (example, NN)], (,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[Well, it, must, 've, been, a, glitch, then, ,...</td>\n",
       "      <td>[(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...</td>\n",
       "      <td>[well, it, must, ve, been, a, glitch, then, ,,...</td>\n",
       "      <td>[(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  [Hitsugaya, being, in, every, fight, is, partl...   \n",
       "1      0  [I, imagine, his, nickname, in, prison, would,...   \n",
       "2      0  [what, about, the, part, where, its, solo, que...   \n",
       "3      0  [Yea, great, example, ,, I, see, Gold, players...   \n",
       "4      0  [Well, it, must, 've, been, a, glitch, then, ,...   \n",
       "\n",
       "                                             POS_tag  \\\n",
       "0  [(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...   \n",
       "1  [(I, PRP), (imagine, VBP), (his, PRP$), (nickn...   \n",
       "2  [(what, WP), (about, IN), (the, DT), (part, NN...   \n",
       "3  [(Yea, NNP), (great, JJ), (example, NN), (,, ,...   \n",
       "4  [(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...   \n",
       "\n",
       "                                                stem  \\\n",
       "0  [hitsugaya, be, in, everi, fight, is, part, be...   \n",
       "1  [i, imagin, his, nicknam, in, prison, would, b...   \n",
       "2  [what, about, the, part, where, it, solo, queu...   \n",
       "3  [yea, great, exampl, ,, i, see, gold, player, ...   \n",
       "4  [well, it, must, ve, been, a, glitch, then, ,,...   \n",
       "\n",
       "                                   noun_phrase_chunk  \n",
       "0  [(Hitsugaya, NNP), (being, VBG), (in, IN), [(e...  \n",
       "1  [(I, PRP), (imagine, VBP), (his, PRP$), [(nick...  \n",
       "2  [(what, WP), (about, IN), [(the, DT), (part, N...  \n",
       "3  [(Yea, NNP), [(great, JJ), (example, NN)], (,,...  \n",
       "4  [(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tag = []\n",
    "for index, row in chosen_data.iterrows():\n",
    "    joined_tag.append(' '.join([word + \"_\" + pos for word, pos in row['POS_tag']]))\n",
    "chosen_data['joined_POS_tag'] = joined_tag.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>stem</th>\n",
       "      <th>noun_phrase_chunk</th>\n",
       "      <th>joined_POS_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Hitsugaya, being, in, every, fight, is, partl...</td>\n",
       "      <td>[(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...</td>\n",
       "      <td>[hitsugaya, be, in, everi, fight, is, part, be...</td>\n",
       "      <td>[(Hitsugaya, NNP), (being, VBG), (in, IN), [(e...</td>\n",
       "      <td>Hitsugaya_NNP being_VBG in_IN every_DT fight_N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[I, imagine, his, nickname, in, prison, would,...</td>\n",
       "      <td>[(I, PRP), (imagine, VBP), (his, PRP$), (nickn...</td>\n",
       "      <td>[i, imagin, his, nicknam, in, prison, would, b...</td>\n",
       "      <td>[(I, PRP), (imagine, VBP), (his, PRP$), [(nick...</td>\n",
       "      <td>I_PRP imagine_VBP his_PRP$ nickname_NN in_IN p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[what, about, the, part, where, its, solo, que...</td>\n",
       "      <td>[(what, WP), (about, IN), (the, DT), (part, NN...</td>\n",
       "      <td>[what, about, the, part, where, it, solo, queu...</td>\n",
       "      <td>[(what, WP), (about, IN), [(the, DT), (part, N...</td>\n",
       "      <td>what_WP about_IN the_DT part_NN where_WRB its_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[Yea, great, example, ,, I, see, Gold, players...</td>\n",
       "      <td>[(Yea, NNP), (great, JJ), (example, NN), (,, ,...</td>\n",
       "      <td>[yea, great, exampl, ,, i, see, gold, player, ...</td>\n",
       "      <td>[(Yea, NNP), [(great, JJ), (example, NN)], (,,...</td>\n",
       "      <td>Yea_NNP great_JJ example_NN ,_, I_PRP see_VBP ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[Well, it, must, 've, been, a, glitch, then, ,...</td>\n",
       "      <td>[(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...</td>\n",
       "      <td>[well, it, must, ve, been, a, glitch, then, ,,...</td>\n",
       "      <td>[(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...</td>\n",
       "      <td>Well_IN it_PRP must_MD 've_VBP been_VBN a_DT g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  [Hitsugaya, being, in, every, fight, is, partl...   \n",
       "1      0  [I, imagine, his, nickname, in, prison, would,...   \n",
       "2      0  [what, about, the, part, where, its, solo, que...   \n",
       "3      0  [Yea, great, example, ,, I, see, Gold, players...   \n",
       "4      0  [Well, it, must, 've, been, a, glitch, then, ,...   \n",
       "\n",
       "                                             POS_tag  \\\n",
       "0  [(Hitsugaya, NNP), (being, VBG), (in, IN), (ev...   \n",
       "1  [(I, PRP), (imagine, VBP), (his, PRP$), (nickn...   \n",
       "2  [(what, WP), (about, IN), (the, DT), (part, NN...   \n",
       "3  [(Yea, NNP), (great, JJ), (example, NN), (,, ,...   \n",
       "4  [(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...   \n",
       "\n",
       "                                                stem  \\\n",
       "0  [hitsugaya, be, in, everi, fight, is, part, be...   \n",
       "1  [i, imagin, his, nicknam, in, prison, would, b...   \n",
       "2  [what, about, the, part, where, it, solo, queu...   \n",
       "3  [yea, great, exampl, ,, i, see, gold, player, ...   \n",
       "4  [well, it, must, ve, been, a, glitch, then, ,,...   \n",
       "\n",
       "                                   noun_phrase_chunk  \\\n",
       "0  [(Hitsugaya, NNP), (being, VBG), (in, IN), [(e...   \n",
       "1  [(I, PRP), (imagine, VBP), (his, PRP$), [(nick...   \n",
       "2  [(what, WP), (about, IN), [(the, DT), (part, N...   \n",
       "3  [(Yea, NNP), [(great, JJ), (example, NN)], (,,...   \n",
       "4  [(Well, IN), (it, PRP), (must, MD), ('ve, VBP)...   \n",
       "\n",
       "                                      joined_POS_tag  \n",
       "0  Hitsugaya_NNP being_VBG in_IN every_DT fight_N...  \n",
       "1  I_PRP imagine_VBP his_PRP$ nickname_NN in_IN p...  \n",
       "2  what_WP about_IN the_DT part_NN where_WRB its_...  \n",
       "3  Yea_NNP great_JJ example_NN ,_, I_PRP see_VBP ...  \n",
       "4  Well_IN it_PRP must_MD 've_VBP been_VBN a_DT g...  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
